# Отчёт по проекту: Go-сервис аналитики нагрузки в Kubernetes

## 1. Титульный лист

_Заполнить_:  
- Название проекта  
- ФИО, группа  
- Курс / дисциплина  
- Дата сдачи  

---

## 2. Введение

- **Цель проекта**: разработать высоконагруженный сервис на Go с простой статистической аналитикой (rolling average + z-score) и развернуть его в Kubernetes с автоматическим масштабированием и мониторингом.  
- **Основные задачи**:
  - Реализовать HTTP‑сервис для приёма метрик (CPU, RPS) и их анализа.
  - Интегрировать Redis для кэширования метрик.
  - Развернуть сервис в Kubernetes (Deployment, Service, Redis, Ingress, HPA).
  - Подключить Prometheus, Grafana и алерты по аномалиям.
  - Провести нагрузочные тесты (≥1000 RPS, 500 RPS / 5 мин) и оценить качество детекции аномалий.
- **Используемый стек**: Go, Redis, Docker, Kubernetes (Minikube/Kind/Killercoda/облако), Prometheus, Grafana, Locust.  
- **Выбор среды**: _кратко описать_, почему выбран Minikube / Kind / Killercoda / облако (ресурсы, удобство, ограничения).  

---

## 3. Архитектура решения

- **Общая схема**:
  - Клиенты → Ingress (NGINX) → Go‑service (Pods) → Redis.
  - Prometheus → сбор метрик с `/metrics`.
  - Grafana → визуализация метрик и аномалий.
  - Alertmanager → оповещения при высоком уровне аномалий.
- **Компоненты**:
  - Go‑сервис: эндпоинты `/ingest`, `/analyze`, `/metrics`, `/healthz`, `/debug/pprof/*`.
  - Redis: кэш последних метрик.
  - Kubernetes‑объекты: Deployment, Service, Ingress, HorizontalPodAutoscaler.
- **Поток данных**:
  - Клиент отправляет метрику в `/ingest` → сервис валидирует и помещает её в канал → горутина обновляет скользящее окно статистики и сохраняет метрику в Redis → `/analyze` возвращает агрегированную статистику по RPS.
- **Аналитика**:
  - Rolling window (окно 50 событий по RPS).
  - Расчёт среднего значения, стандартного отклонения, z‑score; аномалия при |z| > 2; подсчёт `anomaly_count`.

_Сюда же вставить диаграмму архитектуры (сделанную, например, в draw.io / Miro)._  

---

## 4. Реализация сервиса и инфраструктуры

### 4.1. Реализация на Go

- Структура проекта:
  - `cmd/server` — входная точка приложения.
  - `internal/analytics` — реализация rolling window и z-score.
  - `internal/httpserver` — HTTP‑сервер, маршрутизация, middleware, pprof.
  - `internal/storage` — работа с Redis.
  - `internal/metrics` — Prometheus‑метрики.
  - `internal/config` — загрузка конфигурации из переменных окружения.
- Горутины и каналы:
  - Канал `ingestCh` для передачи метрик от HTTP‑слоя к аналитике.
  - Горутина `runIngestLoop` для обработки метрик и записи в Redis.
- Кэширование в Redis:
  - Формат хранения (JSON), ключ (timestamp), время жизни (TTL).

### 4.2. Контейнеризация

- Multi‑stage Dockerfile:
  - Сборка бинарника на базе `golang:1.22-alpine`.
  - Runtime‑образ на базе `alpine`, размер < 300MB.
- Описание команд сборки и запуска образа.

### 4.3. Развёртывание в Kubernetes

- Используемые манифесты (директория `deployments/`):
  - Deployment и Service для Go‑сервиса.
  - Deployment и Service для Redis (либо Helm‑чарт `bitnami/redis`, если использовался).
  - Ingress (маршруты `/metrics` и `/analyze`).
  - HPA (minReplicas, maxReplicas, target CPU).  
- Примеры команд `kubectl apply`, `kubectl get pods` (со скриншотами в Приложении).

---

## 5. Мониторинг и визуализация

- Установка Prometheus и Grafana (через Helm или манифесты):
  - Ссылки на используемые values‑файлы (`prometheus-values.yaml`, `grafana-values.yaml`).
- Настройка сбора метрик:
  - job `go-analyzer`, таргет `go-analyzer:80`.
  - Метрики: `http_requests_total`, `http_request_duration_seconds`, `anomalies_total`.
- Дашборды в Grafana:
  - RPS (например, `rate(http_requests_total[1m])`).
  - Latency (p95 по `http_request_duration_seconds`).
  - Anomaly rate (`increase(anomalies_total[1m])`).
- Alertmanager:
  - Правило оповещения при `increase(anomalies_total[1m]) > 5`.
  - Краткое описание канала доставки (или тестового сценария алерта).

_Сюда добавить ссылки на скриншоты дашбордов и алертов (см. Приложения)._  

---

## 6. Нагрузочное тестирование и результаты аналитики

### 6.1. Методика нагрузочного тестирования

- Инструмент: Locust / Apache Bench (указать фактически использованный).
- Конфигурация:
  - Количество пользователей/запросов в секунду.
  - Профиль нагрузки (разгон, длительность, стабильный уровень).
- Сценарии:
  - Тест производительности: ≥1000 RPS (N минут).
  - Тест стабильности и HPA: 500 RPS в течение ≥5 минут.

### 6.2. Результаты нагрузочного теста

- Таблица с основными метриками:
  - RPS (среднее), p95 latency, процент ошибок, количество реплик (минимум/максимум), средний CPU.  
- Наблюдения:
  - Как менялось число реплик HPA при росте нагрузки.
  - Как изменилась латентность при масштабировании.
- Скриншоты:
  - Из Locust / ab (кривая RPS, ошибки).
  - Из Grafana (RPS, latency, anomaly_rate во время теста).
  - `kubectl get hpa` / `kubectl describe hpa` (рост до 4 реплик).

### 6.3. Оценка качества детекции аномалий

- Описание синтетического сценария:
  - Как генерировались нормальные значения RPS и аномальные пики.
- Таблица:
  - TP, TN, FP, FN, точность, полнота, доля ложных срабатываний.
- Выводы:
  - Достигнуты ли целевые значения (>70% точность, <10% false positive).
  - Какие типы аномалий детектор ловит лучше/хуже всего.

---

## 7. Оптимизация и профилирование

- Использование pprof:
  - Какие профили снимались (CPU, heap).
  - При какой нагрузке проводилось профилирование.
- Выявленные узкие места:
  - Примеры горячих функций или участков кода.
  - Потенциальные проблемы (GC, сериализация JSON, взаимодействие с Redis).
- Проведённые оптимизации:
  - Изменения в конфигурации (буферы, лимиты ресурсов, настройки Redis).
  - Изменения в коде (если были).
- Эффект:
  - Сравнение latency и использования ресурсов до/после.

---

## 8. Заключение

- Краткое резюме выполненных задач:
  - Реализация сервиса на Go (200–300+ строк, рабочий проект).
  - Развёртывание в Kubernetes с HPA, Redis и Ingress.
  - Настройка мониторинга и алертов.
  - Нагрузочные тесты и анализ результатов.
- Основные достижения:
  - Обработка ≥1000 RPS.
  - Устойчивость под нагрузкой (99% uptime в эксперименте, если измерялось).
  - Снижение latency за счёт кэширования/масштабирования (если было подтверждено измерениями).
- Возникшие трудности и их решения:
  - Сетевые/ресурсные ограничения, настройка Ingress, отладка HPA, конфликт версий Helm/чартов и т.д.
- Перспективы развития:
  - Более сложные модели анализа (ML/ARIMA).
  - Интеграция с очередями (Kafka, NATS).
  - Полноценный CI/CD, канареечные релизы, Blue/Green deployment.

---

## 9. Ссылки

- Документация Go, Kubernetes, Prometheus, Grafana, Locust.  
- Ссылка на Git‑репозиторий проекта.  

---

## 10. Приложения

- Полный исходный код (структура репозитория, ключевые файлы).  
- YAML‑манифесты Kubernetes (`deployments/`).  
- Скриншоты:
  - `kubectl get pods`, `kubectl get hpa`, `kubectl get ingress`.
  - Дашборды Grafana (RPS, latency, anomaly_rate).
  - Логи нагрузочного тестирования (Locust/ab).
  - Алерты из Alertmanager (если применимо).  
- Примеры команд для запуска в выбранной среде (Minikube / Kind / Killercoda / облако).

